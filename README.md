# Вопросы к зачету

В этом репозитории выложены вопросы к зачету. Обратите внимание, ответ на вопрос должен быть дан последовательно и развернуто.

Материалы для подготовки к зачету:
1. Книга: Себастьян Рашка "Python и машинное обучение"
2. Учебник от ШАД https://academy.yandex.ru/handbook/ml
3. Книга: С. И. Николенко "Глубокое обучение. Погружение в мир нейронных сетей"
4. Статьи на Хабр, например: https://habr.com/ru/companies/skillfactory/articles/525214/
5. Лучший помощник - гугл

Непосредственно вопросы:
1.	Простейшие операции над матрицами: транспонирование, сложение, умножение на число. Умножение матриц. Как определить, что две матрицы можно умножить. Размер выходной матрицы.
2.	Производная. Производная сложной функции. Градиент. Градиенты простейших функций: MSE, Binary Cross-Entropy
3.	Схема Бернулли. Теорема Байеса. Виды статистических распределений
4.	Правдоподобие. Вывод отрицательного логарифмического правдоподобия в случае бинарной классификации

Непосредственно ML и инструменты:
5.	Пропуски в данных. Как работать с пропусками. Привести примеры
6.	Выбросы в данных. Как работать с выбросами
7.	Numpy. Операции над тензорами в numpy
8.	Loss-функция. Определение. Смысл. Примеры простейших loss-функций и их смысл
9.	Обучение модели машинного обучения. Алгоритм градиентного спуска. Стохастический градиентный спуск. Пакетный градиентный спуск. 
10.	Проблемы недо- и переобучения. Проблема локальных минимумов и ее решение. Идея разделения выборки на train, val, test. Как выявить переобучение
11.	Продвинутые алгоритмы обучения моделей машинного обучения. Nesterov Accelerated Gradients, Adagrad, RMSProp, Adam
12.	Простейшие метрики качества моделей в случае задач регрессии и классификации и их интерпретация
13.	Регуляризация. L1 и L2 штрафы. Градиент MSE с L1 и L2 штрафами. Применение регуляризации
14.	Линейная регрессия. Логистическая регрессия. Матричные формулы для получения отклика модели
15.	Интерпретация параметров линейных моделей. Feature-engineering
16.	Обобщение линейной модели на n выходных значений
17.	Деревья решений. Обучение деревьев решений. Гиперпараметры деревьев решений
18.	K-ближайших соседей. Идея алгоритма. Гиперпараметры
19.	Случайный лес. Сэмплинг обучающих данных. Предсказание результата случайным лесом. Гиперпараметры алгоритма
20.	Градиентный бустинг. На каких алгоритмах можно построить.  Гиперпараметры
21.	Полносвязные нейронные сети. Прямое распространение
22.	Функция активации. Виды функций активации
23.	Понятие графа вычислений
24.	Обучение полносвязной нейронной сети. Обратное распространение ошибки
25.	Проблема «взрывающегося» и затухающего градиента
26.	Проблема исключающего «или»
