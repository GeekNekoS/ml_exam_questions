### Правдоподобие. Вывод отрицательного логарифмического правдоподобия в случае бинарной классификации

1. `Правдоподобие`

Правдоподобие — это способ измерить, насколько хорошо наша модель объясняет данные. Представьте, что у нас есть догадка 
о том, как устроены данные (модель), и мы хотим узнать, насколько эта догадка близка к реальности. Правдоподобие 
показывает, насколько вероятно, что наша модель могла бы предсказать те данные, которые мы видим. Чем выше 
правдоподобие, тем точнее модель отражает данные.

2. `Отрицательное логарифмическое правдоподобие (NLL) в бинарной классификации`

Бинарная классификация — это когда у нас есть всего два возможных исхода для каждого объекта, например, 0 и 1 (например,
больной или здоровый). Наша модель предсказывает вероятность каждого исхода, например, вероятность того, что человек 
болен, может быть 0.8 (80%).

Когда мы хотим оценить, насколько хорошо модель предсказывает такие вероятности, мы используем правдоподобие. Но чтобы 
считать его удобнее, мы берем логарифм правдоподобия и меняем знак на минус — это и есть отрицательное логарифмическое 
правдоподобие (NLL).

Формула NLL выглядит так:

![img.png](../ml_exam_questions/service_files/img9.png)

где:

 - ![img.png](../ml_exam_questions/service_files/img10.png) — это истинный результат (0 или 1),
 - ![img.png](../ml_exam_questions/service_files/img11.png)— вероятность, которую предсказала модель для того, что ![img.png](../ml_exam_questions/service_files/img10.png) = 1.

#### Зачем это нужно?

Мы минимизируем NLL, чтобы модель предсказывала результаты более точно. NLL "штрафует" модель, если она сильно 
ошибается в своих предсказаниях:

 - Если модель предсказала вероятность 0.9 для истинного класса 1, то она получает маленький штраф, так как была почти 
права.
 - Если модель предсказала вероятность 0.1 для истинного класса 1, штраф будет большим, так как предсказание далеко 
от реальности.

Итог: Минимизируя NLL, мы стремимся к тому, чтобы модель делала более точные предсказания, особенно для тех случаев, 
где предсказанные вероятности сильно расходятся с реальными данными.
